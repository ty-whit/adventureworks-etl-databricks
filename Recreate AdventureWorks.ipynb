{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a69a2718-a183-4360-801c-f10917be82dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Recreating the AdventureWorks database. \n",
    "In this notebook, we perform all steps needed to \n",
    "1. clean out any previous iteration of the adventureworks database. \n",
    "2. download a zip file of the Adventure Works Database provided in parquet format. \n",
    "3. use that zip file to create tables in the `adventureworks` schema. \n",
    "\n",
    "Due to the limitations of the Databricks Free edition, I do not seem to have the ability to create seperate workspaces required to properly create the seperate schemas needed for a proper AdventureWorks database. For this reason, all tables have the prefix of the correct schema, e.g., what should be `adventureworks.person.emailaddress` is can be found at `workspace.adventureworks.person_emailaddress`. I will call out later in the script where this script should be modified to properly create the schema. \n",
    "\n",
    "This script should be run whenever a user wants to create the `adventureworks` database within a databricks system workspace, or wants to recreate/reset the tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e239be1f-1c51-445c-9775-0d31ecd362e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Initial Setup\n",
    "We start with importing all the libraries we are going to need for this script and ensure we are using the correct catalog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b452f90-33e6-4f7d-aa7c-ce3033419ea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType, FloatType\n",
    "import os\n",
    "import requests\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ebf821e-e1be-4dfc-bc1e-64d062118dcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "use catalog workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "654a9cb2-670e-4654-9a40-08404a185292",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "*Note*: The above cell needs to be changed depending on your catalog name. It likely is going to chage if you are able to create a catalog for `adventureworks`, you will likely also have the ability to create seperate schams, which is discussed more later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a05ddd1f-8484-4ab0-bed9-04e04ff6cbc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Clean-up\n",
    "Next we grab all schemas related to adventureworks and drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cc30cf1-2495-4723-a06e-18dea5f2a1ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schemas = spark.sql('''\n",
    "select schema_name\n",
    "from `information_schema`.`schemata` \n",
    "where schema_name like \"adventureworks%\"\n",
    "limit 100;\n",
    "''')\n",
    "\n",
    "for schema in _sqldf.collect():\n",
    "    spark.sql(f'''DROP SCHEMA IF EXISTS {schema.schema_name} CASCADE''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa810a2e-1b8f-405a-9dbc-77bd44722aed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Recreating Schema and Volume\n",
    "Next, we recreate the namespaces used by this script. Specifcaly, we create the `adventureworks`, set it to the default, then create a volume within that schema to store the raw data files we are going to be using. \n",
    "*Note*: This is one of the areas that you can change if are using multiple catalogs. If you can create a `adventureworks` catalog, you would be able to create seperate schemas for each schema as defined by the original AdventureWorks sample database. This script would change, depending on that ability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4843aa65-dd94-431c-aadb-6375003ccf29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%sql\n",
    "CREATE SCHEMA adventureworks;\n",
    "USE SCHEMA adventureworks;\n",
    "CREATE VOLUME adventureworks.raw_data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c16d31f-bbc2-4227-a33d-677f327004b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Next, we set a handful of variables we will be using to store files. \n",
    "*Note*: If you have the ability to create seperate catalogs, make sure you change your paths here as well.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4940aad-5476-4d66-a52d-e8f5b3cc0705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = 'adventureworks'\n",
    "volume = 'raw_data'\n",
    "url = 'https://github.com/olafusimichael/AdventureWorksParquet/archive/main.zip'\n",
    "workspace_dir = f'/Volumes/workspace/{schema}/raw_data'\n",
    "# parquet_dir = f'{workspace_dir}'\n",
    "zip_path = f'{workspace_dir}/main.zip'\n",
    "# dbutils.fs.mkdirs(parquet_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "676d60d3-f2e9-47b6-8eab-b92453fc0f28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "At this point, we download the zip file, as provided by [Michael Olafusi (olafusimichael)](https://github.com/olafusimichael/AdventureWorksParquet) and save all data to our newly created volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56b6d4c3-9f98-4cbd-bade-3d7840b2fe66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(f'{workspace_dir}/main.zip', 'wb') as f:\n",
    "    f.write(requests.get(url).content)\n",
    "with zipfile.ZipFile(f'{workspace_dir}/main.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(workspace_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8ba2c15-17ca-4503-bd21-fdf7d3871e52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Table Creation\n",
    "This is the meat and potatoes of this script. With everything else done, we loop through all parquete files, and create a table out of the data within. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12b6b08f-1894-4466-b40b-7d92a8861148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parquet_dir = f'{workspace_dir}/AdventureWorksParquet-main'\n",
    "for file in dbutils.fs.ls(parquet_dir):\n",
    "    if file.name.endswith('.parquet'):\n",
    "        table_name = file.name.replace('.parquet', '').replace(\".\", \"_\")\n",
    "        df = spark.read.parquet(file.path)\n",
    "        df.write.mode('overwrite').saveAsTable(f'{schema}.{table_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbed0d5f-eeaf-414c-b5da-85b8fe2642f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Once completed, you should be able to see all the tables in the `adventureworks` schema of your workspace. "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4862064725444787,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Recreate AdventureWorks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
