{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a69a2718-a183-4360-801c-f10917be82dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Recreating the AdventureWorks database\n",
    "In this notebook, we perform all steps needed to \n",
    "1. clean out any previous iteration of the adventureworks database. \n",
    "2. download a zip file of the Adventure Works Database provided in parquet format. \n",
    "3. use that zip file to create tables in the `adventureworks` schema. \n",
    "\n",
    "This script should be run whenever a user wants to create the `adventureworks` database for the first time or reset the all \n",
    "tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e239be1f-1c51-445c-9775-0d31ecd362e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Initial Setup\n",
    "We start with importing all the libraries we are going to need for this script and ensure we are using the correct catalog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b452f90-33e6-4f7d-aa7c-ce3033419ea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType, FloatType\n",
    "import os\n",
    "import requests\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a674755-9ef3-4293-bae7-2ab549578bd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup and Recreate\n",
    "In this first step, we begin by completely dropping and recreating the `adventureworks` catalog. This is going to simulate the sample database provided by Microsoft. This should be renamed to whatever database/catalog you wish to refer to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ebf821e-e1be-4dfc-bc1e-64d062118dcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "DROP CATALOG IF EXISTS adventureworks CASCADE;\n",
    "CREATE CATALOG adventureworks;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ca9eb9e-1b4e-4680-85d4-67bfbec80aa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Download Data\n",
    "With the catalog set up We now have to create the volume filesystem for our flat files. We also initiate the download to get our pipeline moving along. The files used in this pipeline were provided by [Michael Olafusi (olafusimichael/AdventureWorksParquet)](https://github.com/olafusimichael/AdventureWorksParquet).\n",
    "\n",
    "All these values should be renamed if using a different naming convention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4940aad-5476-4d66-a52d-e8f5b3cc0705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = 'adventureworks'\n",
    "schema = 'default'\n",
    "volume = 'adventureworks_parquet'\n",
    "url = 'https://github.com/olafusimichael/AdventureWorksParquet/archive/main.zip'\n",
    "workspace_dir = f'/Volumes/{catalog}/{schema}/{volume}'\n",
    "zip_path = f'{workspace_dir}/main.zip'\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE VOLUME IF NOT EXISTS {catalog}.{schema}.{volume}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56b6d4c3-9f98-4cbd-bade-3d7840b2fe66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(zip_path, 'wb') as f:\n",
    "    f.write(requests.get(url).content)\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(workspace_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8ba2c15-17ca-4503-bd21-fdf7d3871e52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Table Creation\n",
    "This is the meat and potatoes of this script. With everything else done, we loop through all parquete files, and create a table out of the data within. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12b6b08f-1894-4466-b40b-7d92a8861148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parquet_dir = f'{workspace_dir}/AdventureWorksParquet-main'\n",
    "for file in dbutils.fs.ls(parquet_dir):\n",
    "    if file.name.endswith('.parquet'):\n",
    "        table_name = file.name.replace('.parquet', '')\n",
    "        schema_name = table_name.split('.')[0]\n",
    "        spark.sql(f'CREATE SCHEMA IF NOT EXISTS {catalog}.{schema_name}')\n",
    "        df = spark.read.parquet(file.path)\n",
    "        df.write.mode('overwrite').saveAsTable(f'{catalog}.{table_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be2590b7-eb5a-4e99-bde7-4db0564af662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Adding missing files\n",
    "A couple files seem to be missing from the above mentioned parquet dataset. Therefore, I'll be importing the 2022 version of the CSV files from microsoft and add them in as I need more tables for the data warehouse.\n",
    "I've attempted to create these from the provided install scripts inside the zip file provided by microsoft below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c970ca8f-a9b9-4915-933a-b5174bfd0fbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f'''\n",
    "    CREATE VOLUME IF NOT EXISTS {catalog}.{schema}.csv_files\n",
    "    ''')\n",
    "csv_url = 'https://github.com/Microsoft/sql-server-samples/releases/download/adventureworks/AdventureWorks-oltp-install-script.zip'\n",
    "csv_workspace_dir = f'/Volumes/{catalog}/{schema}/csv_files'\n",
    "zip_path_csv = f'{csv_workspace_dir}/AdventureWorksCSV.zip'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f3e0277-8242-4ea6-9600-48fb5ba32f8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(zip_path_csv, 'wb') as f:\n",
    "    f.write(requests.get(csv_url).content)\n",
    "with zipfile.ZipFile('/Volumes/adventureworks/default/csv_files/AdventureWorksCSV.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(csv_workspace_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b26f6bea-a66f-4181-846c-db89fd10d2fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_schema = StructType([\n",
    "    StructField('EmployeeID', IntegerType(), True),\n",
    "    StructField('NationalIDNumber', StringType(), True),\n",
    "    StructField('LoginID', StringType(), True),\n",
    "    StructField('OrganizationNode', StringType(), True),\n",
    "    StructField('OrganizationLevel', IntegerType(), True),\n",
    "    StructField('JobTitle', StringType(), True),\n",
    "    StructField('BirthDate', DateType(), True),\n",
    "    StructField('MaritalStatus', StringType(), True),\n",
    "    StructField('Gender', StringType(), True),\n",
    "    StructField('HireDate', DateType(), True),\n",
    "    StructField('SalariedFlag', IntegerType(), True),\n",
    "    StructField('VacationHours', IntegerType(), True),\n",
    "    StructField('SickLeaveHours', IntegerType(), True),\n",
    "    StructField('CurrentFlag', IntegerType(), True),\n",
    "    StructField('rowguid', StringType(), True),\n",
    "    StructField('ModifiedDate', DateType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(f'{csv_workspace_dir}/Employee.csv',header=False,schema=df_schema,inferSchema=True,sep='\\t',encoding='utf-16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5ac29b6-8ba6-499a-aa94-c6c837214486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS adventureworks.humanresources.employee\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea2cc1bb-f4b0-4b5a-9b0c-f7498c687323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode('overwrite').saveAsTable(\"adventureworks.humanresources.employee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39a607d6-3c5e-4efa-896c-d1c3b6f581b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select EmployeeID from adventureworks.humanresources.employee\").count() == df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbed0d5f-eeaf-414c-b5da-85b8fe2642f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Once completed, you should be able to see all the tables in the `adventureworks` catalog of your workspace, inside their respective schemas. This will effectively turn the `adventureworks` catalog into a sample database for any further queries. This verison of the database does lack, any simblance of partitions, indicies, primary and foreign keys, constraints, and many other standard database features. These could likely be included, especially if utilizing the upcoming [Lakebase](https://docs.databricks.com/aws/en/oltp) feature I found, but more efford would need to be put into importing these files, which is not the point of this repo and its exercises. "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5041068088577523,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Recreate AdventureWorks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
