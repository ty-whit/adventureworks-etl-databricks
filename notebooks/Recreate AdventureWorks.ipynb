{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a69a2718-a183-4360-801c-f10917be82dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Recreating the AdventureWorks database\n",
    "In this notebook, we perform all steps needed to \n",
    "1. clean out any previous iteration of the adventureworks database. \n",
    "2. download a zip file of the Adventure Works Database provided in parquet format. \n",
    "3. use that zip file to create tables in the `adventureworks` schema. \n",
    "\n",
    "This script should be run whenever a user wants to create the `adventureworks` database for the first time or reset the all \n",
    "tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e239be1f-1c51-445c-9775-0d31ecd362e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Initial Setup\n",
    "We start with importing all the libraries we are going to need for this script and set up a common funciton used for creating tables from CSV files as needed. \n",
    "\n",
    "Specifically, we use \n",
    "* `requests` to download files from GitHub \n",
    "* `zipfile` to unpack the zip files we downloaded\n",
    "* and various classes from `pyspark.sql.types` to define the `StructType` which gets passed the PySpark method to read csv. This object allows us to define both the column names and types when reading csv files to avoid ambiguity. \n",
    "\n",
    "We also define the `create_csv_table` to combine the shared logic needed for all CSV files that need to be read in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b452f90-33e6-4f7d-aa7c-ce3033419ea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType, FloatType\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "\n",
    "def create_csv_table(schema, table_name, df_schema, sep=',', line_sep='\\n', encoding='utf-16'): \n",
    "    \"\"\"\n",
    "    Creates a Spark table from a CSV file using the specified schema, file name, and file options.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    schema : str\n",
    "        The schema name where the table will be created.\n",
    "    table_name : str\n",
    "        The name of the table to be created.\n",
    "    df_schema : StructType\n",
    "        The schema to apply when reading the CSV file.\n",
    "    sep : str, optional\n",
    "        The delimiter to use in the CSV file (default is ',').\n",
    "    line_sep : str, optional\n",
    "        The line separator to use in the CSV file (default is '\\n').\n",
    "    encoding : str, optional\n",
    "        The encoding of the CSV file (default is 'utf-16').\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the row count in the created table does not match the row count in the DataFrame.\n",
    "    Exception\n",
    "        If there is an error reading the table or DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    catalog = 'adventureworks'\n",
    "    csv_workspace_dir = f'/Volumes/{catalog}/default/csv_files'\n",
    "\n",
    "    df = spark.read.csv(f'{csv_workspace_dir}/{table_name}.csv',header=False,schema=df_schema,inferSchema=True,sep='\\t',encoding='utf-16')\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{schema}.{table_name.lower()}\")\n",
    "    df.write.mode('overwrite').saveAsTable(f\"{catalog}.{schema}.{table_name.lower()}\")\n",
    "    spark.sql(f\"select * from {catalog}.{schema}.{table_name.lower()}\").count() == df.count()   \n",
    "    \n",
    "    # Try to read table first.\n",
    "    try:\n",
    "        table_count = spark.sql(f\"SELECT COUNT(*) FROM {catalog}.{schema}.{table_name.lower()}\").collect()[0][0]\n",
    "        df_count = df.count()\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    \n",
    "    # Compare row counts, raise error on count mismatch\n",
    "    if table_count == df_count:\n",
    "        print(f\"Table {catalog}.{schema}.{table_name.lower()} created successfully\")\n",
    "    else:\n",
    "        raise ValueError(f\"Row count mismatch: table={table_count.lower()}, dataframe={df_count}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a674755-9ef3-4293-bae7-2ab549578bd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup and Recreate\n",
    "In this first step, we begin by completely dropping and recreating the `adventureworks` catalog. This is going to simulate the sample database provided by Microsoft. This should be renamed to whatever database/catalog you wish to refer to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ebf821e-e1be-4dfc-bc1e-64d062118dcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "DROP CATALOG IF EXISTS adventureworks CASCADE;\n",
    "CREATE CATALOG adventureworks;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ca9eb9e-1b4e-4680-85d4-67bfbec80aa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Download Data\n",
    "With the catalog set up We now have to create the volume filesystem for our flat files. We also initiate the download to get our pipeline moving along. The files used in this pipeline were provided by [Michael Olafusi (olafusimichael/AdventureWorksParquet)](https://github.com/olafusimichael/AdventureWorksParquet).\n",
    "\n",
    "All these values should be renamed if using a different naming convention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4940aad-5476-4d66-a52d-e8f5b3cc0705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = 'adventureworks'\n",
    "schema = 'default'\n",
    "volume = 'adventureworks_parquet'\n",
    "url = 'https://github.com/olafusimichael/AdventureWorksParquet/archive/main.zip'\n",
    "workspace_dir = f'/Volumes/{catalog}/{schema}/{volume}'\n",
    "zip_path = f'{workspace_dir}/main.zip'\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "    CREATE VOLUME IF NOT EXISTS {catalog}.{schema}.{volume}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56b6d4c3-9f98-4cbd-bade-3d7840b2fe66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(zip_path, 'wb') as f:\n",
    "    f.write(requests.get(url).content)\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(workspace_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8ba2c15-17ca-4503-bd21-fdf7d3871e52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Table Creation\n",
    "This is the meat and potatoes of this script. With everything else done, we loop through all parquete files, and create a table out of the data within. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12b6b08f-1894-4466-b40b-7d92a8861148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parquet_dir = f'{workspace_dir}/AdventureWorksParquet-main'\n",
    "for file in dbutils.fs.ls(parquet_dir):\n",
    "    if file.name.endswith('.parquet'):\n",
    "        table_name = file.name.replace('.parquet', '')\n",
    "        schema_name = table_name.split('.')[0]\n",
    "        spark.sql(f'CREATE SCHEMA IF NOT EXISTS {catalog}.{schema_name}')\n",
    "        df = spark.read.parquet(file.path)\n",
    "        df.write.mode('overwrite').saveAsTable(f'{catalog}.{table_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be2590b7-eb5a-4e99-bde7-4db0564af662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Adding missing tables\n",
    "A couple files seem to be missing from the above mentioned parquet dataset. Therefore, I'll be importing the 2022 version of the CSV files from microsoft and add them in as I need more tables for the data warehouse.\n",
    "I've attempted to create these from the provided install scripts inside the zip file provided by microsoft below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c970ca8f-a9b9-4915-933a-b5174bfd0fbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    f'''\n",
    "    CREATE VOLUME IF NOT EXISTS {catalog}.{schema}.csv_files\n",
    "    ''')\n",
    "csv_url = 'https://github.com/Microsoft/sql-server-samples/releases/download/adventureworks/AdventureWorks-oltp-install-script.zip'\n",
    "csv_workspace_dir = f'/Volumes/{catalog}/{schema}/csv_files'\n",
    "zip_path_csv = f'{csv_workspace_dir}/AdventureWorksCSV.zip'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f3e0277-8242-4ea6-9600-48fb5ba32f8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(zip_path_csv, 'wb') as f:\n",
    "    f.write(requests.get(csv_url).content)\n",
    "with zipfile.ZipFile('/Volumes/adventureworks/default/csv_files/AdventureWorksCSV.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(csv_workspace_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e0ebd24-d7fc-4fd8-80a2-1a7e5427476e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Employee\n",
    "First up is the employee table. Luckly this one is simple enough with tab seperated columns, and new lines seperating the rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b26f6bea-a66f-4181-846c-db89fd10d2fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_schema = StructType([\n",
    "    StructField('EmployeeID', IntegerType(), True),\n",
    "    StructField('NationalIDNumber', StringType(), True),\n",
    "    StructField('LoginID', StringType(), True),\n",
    "    StructField('OrganizationNode', StringType(), True),\n",
    "    StructField('OrganizationLevel', IntegerType(), True),\n",
    "    StructField('JobTitle', StringType(), True),\n",
    "    StructField('BirthDate', DateType(), True),\n",
    "    StructField('MaritalStatus', StringType(), True),\n",
    "    StructField('Gender', StringType(), True),\n",
    "    StructField('HireDate', DateType(), True),\n",
    "    StructField('SalariedFlag', IntegerType(), True),\n",
    "    StructField('VacationHours', IntegerType(), True),\n",
    "    StructField('SickLeaveHours', IntegerType(), True),\n",
    "    StructField('CurrentFlag', IntegerType(), True),\n",
    "    StructField('rowguid', StringType(), True),\n",
    "    StructField('ModifiedDate', DateType(), True)\n",
    "])\n",
    "\n",
    "create_csv_table('person', 'Employee', df_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d14bd41-0f1f-46db-bff1-e3876bf57482",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Address\n",
    "Next up is the `person.address`. Again, a tab seperated columns with new lines on the rows. Essentially repeating the above. I should consider a function to perform the same steps above over and over again. That can be on the TODO list for future interations of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a97ec59-6dca-4c16-905d-8e3db1258114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_schema = StructType([\n",
    "    StructField('AddressID', IntegerType(), True),\n",
    "    StructField('AddressLine1', StringType(), True),\n",
    "    StructField('AddressLine2', StringType(), True),\n",
    "    StructField('City', StringType(), True),\n",
    "    StructField('StateProvinceID', IntegerType(), True),\n",
    "    StructField('PostalCode', StringType(), True),\n",
    "    StructField('SpatialLocation', StringType(), True),\n",
    "    StructField('rowguid', StringType(), True),\n",
    "    StructField('ModifiedDate', DateType(), True)\n",
    "])\n",
    "\n",
    "create_csv_table('humanresources', 'Address', df_schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbed0d5f-eeaf-414c-b5da-85b8fe2642f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Once completed, you should be able to see all the tables in the `adventureworks` catalog of your workspace, inside their respective schemas. This will effectively turn the `adventureworks` catalog into a sample database for any further queries. This verison of the database does lack, any simblance of partitions, indicies, primary and foreign keys, constraints, and many other standard database features. These could likely be included, especially if utilizing the upcoming [Lakebase](https://docs.databricks.com/aws/en/oltp) feature I found, but more efford would need to be put into importing these files, which is not the point of this repo and its exercises. "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4862064725444785,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Recreate AdventureWorks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
